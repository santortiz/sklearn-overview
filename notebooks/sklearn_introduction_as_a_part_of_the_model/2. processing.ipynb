{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already learned how the efficiency of a model is significantly affected by pre-processing steps on data before training.\n",
    "\n",
    "This leads us to think of a model not only as the sklearn-object but the pipeline of pre-processing steps plus the sklearn-object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_model = KNeighborsClassifier(n_jobs=-1, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('model', isolated_model)\n",
    "])\n",
    "\n",
    "pipe_minmax = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('model', isolated_model)\n",
    "])\n",
    "\n",
    "pipe_standard = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', isolated_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "\n",
    "def run_pipe():\n",
    "    pipe.fit(X, y)\n",
    "\n",
    "def run_pipe_minmax():\n",
    "    pipe_minmax.fit(X, y)\n",
    "\n",
    "def run_pipe_standard():\n",
    "    pipe_standard.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7447044880000249"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(run_pipe, number=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0371052380000947"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(run_pipe_minmax, number=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.079402636000168"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(run_pipe_standard, number=n_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, for experimentation, pipelines are not the most efficient workflow since it executes all steps every time. This leads to an increase of execution times.\n",
    "\n",
    "BUT, pipelines are great for creating structures that contain all the information about how a machine learning model is being trained.\n",
    "In the next notebook we'll see a great application for finding out the best configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-mxAdRHL7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
